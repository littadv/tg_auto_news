"""
–ü–∞—Ä—Å–µ—Ä –¥–ª—è bcs-express.ru

–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ bcs-express.ru.
"""

import asyncio
import random
import re
import logging
from typing import List, Tuple, Optional
import httpx

from .base_html import BaseHTMLParser
from config.settings import Settings
from utils.http_client import get_browser_headers


class BCSParser(BaseHTMLParser):
    """
    –ü–∞—Ä—Å–µ—Ä –¥–ª—è bcs-express.ru
    
    –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–∏ —Å —Å–∞–π—Ç–∞ bcs-express.ru —á–µ—Ä–µ–∑ RSS –∏ HTML.
    """
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è BCS –ø–∞—Ä—Å–µ—Ä–∞
    RSS_URLS = [
        "https://www.bcs-express.ru/news?format=rss",
        "https://bcs-express.ru/news?format=rss",
    ]
    
    SITE_ROOTS = [
        "https://www.bcs-express.ru", 
        "https://bcs-express.ru"
    ]
    
    HOMEPAGES = [root + "/" for root in SITE_ROOTS]
    
    def __init__(
        self,
        settings: Settings,
        http_client: httpx.AsyncClient,
        message_sender,
        deduplication_manager,
        date_checker,
        logger: Optional[logging.Logger] = None,
        error_callback=None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BCS –ø–∞—Ä—Å–µ—Ä–∞
        
        Args:
            settings: –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
            http_client: HTTP –∫–ª–∏–µ–Ω—Ç
            message_sender: –û—Ç–ø—Ä–∞–≤–∏—Ç–µ–ª—å —Å–æ–æ–±—â–µ–Ω–∏–π
            deduplication_manager: –ú–µ–Ω–µ–¥–∂–µ—Ä –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            date_checker: –ü—Ä–æ–≤–µ—Ä—è–ª—å—â–∏–∫ –¥–∞—Ç
            logger: –õ–æ–≥–≥–µ—Ä
            error_callback: –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—à–∏–±–æ–∫
        """
        super().__init__(
            name="BCS Parser",
            base_url="https://www.bcs-express.ru",
            message_sender=message_sender,
            deduplication_manager=deduplication_manager,
            date_checker=date_checker,
            logger=logger,
            error_callback=error_callback
        )
        
        self.settings = settings
        self.http_client = http_client
        self.source_name = "www.bcs-express.ru"
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç BCS –ø–∞—Ä—Å–µ—Ä"""
        try:
            if self.logger:
                self.logger.info("–ó–∞–ø—É—Å–∫ BCS –ø–∞—Ä—Å–µ—Ä–∞")
            
            self._running = True
            
            # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞
            while self._running:
                try:
                    await self._parse_cycle()
                    
                    # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º —Ü–∏–∫–ª–æ–º
                    await self.sleep_with_jitter(self.settings.request_timeout)
                    
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    self.increment_error_count()
                    
                    error_msg = (
                        f"üö® <b>BCS Parser Error</b>\n\n"
                        f"‚ùå <b>Source:</b> {self.source_name}\n"
                        f"‚ùå <b>Error:</b> {type(e).__name__}\n"
                        f"üìù <b>Details:</b> {str(e)[:200]}"
                    )
                    
                    await self.send_error(error_msg)
                    
                    if self.logger:
                        self.logger.error(f"–û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ BCS: {e}")
                    
                    # –ñ–¥–µ–º –¥–æ–ª—å—à–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
                    await self.sleep_with_jitter(self.settings.request_timeout * 2)
            
            if self.logger:
                self.logger.info("BCS –ø–∞—Ä—Å–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                
        except Exception as e:
            if self.logger:
                self.logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ BCS –ø–∞—Ä—Å–µ—Ä–∞: {e}")
            raise
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç BCS –ø–∞—Ä—Å–µ—Ä"""
        self._running = False
    
    async def _parse_cycle(self):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç –æ–¥–∏–Ω —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        try:
            # –ü–∞—Ä—Å–∏–º –Ω–æ–≤–æ—Å—Ç–∏
            news_items = await self.parse_news_items(self.http_client)
            
            if not news_items:
                if self.logger:
                    self.logger.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ BCS")
                # –î–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫—É "–æ—Ç–¥–æ—Ö–Ω—É—Ç—å": –Ω–µ –¥–µ—Ä–≥–∞–µ–º –µ–≥–æ —Ö–æ—Ç—è –±—ã –º–∏–Ω—É—Ç—É
                await asyncio.sleep(max(60, int(self.settings.request_timeout)))
                return
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é –Ω–æ–≤–æ—Å—Ç—å
            processed_count = 0
            for title, link, pub_date in news_items:
                try:
                    success = await self.process_news_item(
                        title=title,
                        content="",  # –î–ª—è BCS –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–æ–∫
                        link=link,
                        raw_date=pub_date,
                        source=self.source_name
                    )
                    
                    if success:
                        processed_count += 1
                        
                except Exception as e:
                    if self.logger:
                        self.logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ BCS: {e}")
                    continue
            
            if self.logger and processed_count > 0:
                self.logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed_count} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ BCS")
                
        except Exception as e:
            if self.logger:
                self.logger.error(f"–û—à–∏–±–∫–∞ —Ü–∏–∫–ª–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ BCS: {e}")
            raise
    
    async def parse_news_items(self, http_client: httpx.AsyncClient) -> List[Tuple[str, str, Optional[str]]]:
        """
        –ü–∞—Ä—Å–∏—Ç –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å BCS —Å–∞–π—Ç–∞
        
        Args:
            http_client: HTTP –∫–ª–∏–µ–Ω—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–∑–∞–≥–æ–ª–æ–≤–æ–∫, —Å—Å—ã–ª–∫–∞, –¥–∞—Ç–∞)
        """
        items = []
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º RSS
        rss_items = await self._parse_rss_feeds(http_client)
        if rss_items:
            items = rss_items
        else:
            # Fallback –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            items = await self._parse_homepage(http_client)
        
        return items
    
    async def _parse_rss_feeds(self, http_client: httpx.AsyncClient) -> List[Tuple[str, str, Optional[str]]]:
        """
        –ü–∞—Ä—Å–∏—Ç RSS –ª–µ–Ω—Ç—ã BCS
        
        Args:
            http_client: HTTP –∫–ª–∏–µ–Ω—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ RSS
        """
        for rss_url in self.RSS_URLS:
            try:
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                headers = get_browser_headers(accept_xml=True)
                host_root = rss_url.split("/news", 1)[0]
                headers["Referer"] = host_root + "/"
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º RSS
                response = await self.fetch_url(http_client, rss_url, headers)
                if not response:
                    continue
                
                # –ü–∞—Ä—Å–∏–º RSS
                items = self._parse_rss_xml(response.text, limit=12)
                if items:
                    if self.logger:
                        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π –≤ RSS BCS")
                    return items
                
            except Exception as e:
                if self.logger:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS BCS {rss_url}: {e}")
                continue
        
        return []
    
    def _parse_rss_xml(self, xml: str, limit: int = 12) -> List[Tuple[str, str, Optional[str]]]:
        """
        –ü–∞—Ä—Å–∏—Ç XML RSS –ª–µ–Ω—Ç—ã
        
        Args:
            xml: XML —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ RSS
            limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        items = re.findall(r"<item>(.*?)</item>", xml, re.DOTALL | re.IGNORECASE)[:limit]
        result = []
        
        for chunk in items:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            title_match = re.search(r"<title>(.*?)</title>", chunk, re.DOTALL | re.IGNORECASE)
            title = self.clean_html(title_match.group(1)) if title_match else ""
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫—É
            link_match = re.search(r"<link>(.*?)</link>", chunk, re.DOTALL | re.IGNORECASE)
            link = self.clean_html(link_match.group(1)) if link_match else self.SITE_ROOTS[0]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
            pub_match = re.search(r"<pubDate>(.*?)</pubDate>", chunk, re.DOTALL | re.IGNORECASE)
            pub_date = self.clean_html(pub_match.group(1)) if pub_match else None
            
            if title:
                result.append((title, link, pub_date))
        
        return result
    
    async def _parse_homepage(self, http_client: httpx.AsyncClient) -> List[Tuple[str, str, Optional[str]]]:
        """
        –ü–∞—Ä—Å–∏—Ç –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É BCS
        
        Args:
            http_client: HTTP –∫–ª–∏–µ–Ω—Ç
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        """
        for site_root, homepage in zip(self.SITE_ROOTS, self.HOMEPAGES):
            try:
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                headers = get_browser_headers(accept_xml=False)
                headers["Referer"] = site_root + "/"
                
                # –ó–∞–≥—Ä—É–∂–∞–µ–º –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                response = await self.fetch_url(http_client, homepage, headers)
                if not response:
                    continue
                
                # –ü–∞—Ä—Å–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                titles = self.extract_titles_from_html(response.text, limit=10)
                if titles:
                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
                    items = [(title, site_root, None) for title in titles]
                    if self.logger:
                        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(items)} –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ –≥–ª–∞–≤–Ω–æ–π BCS")
                    return items
                
            except Exception as e:
                if self.logger:
                    self.logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥–ª–∞–≤–Ω–æ–π BCS {site_root}: {e}")
                continue
        
        return []


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
async def bcs_parser(
    httpx_client: httpx.AsyncClient,
    posted_q,
    n_test_chars: int = 50,
    timeout: int = 2,
    –ø—Ä–æ–≤–µ—Ä–∫–∞_–¥–∞—Ç—ã=None,
    send_message_func=None,
    logger=None,
    error_callback=None,
):
    """
    –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º
    
    –ü–∞—Ä—Å–∏—Ç BCS —Å–∞–π—Ç –≤ —Å—Ç–∞—Ä–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.
    """
    import asyncio
    import random
    import re
    from collections import deque
    from typing import List, Tuple
    from ...utils.http_client import get_browser_headers
    from ...utils.date_checker import date_checker
    
    RSS_URLS = [
        "https://www.bcs-express.ru/news?format=rss",
        "https://bcs-express.ru/news?format=rss",
    ]
    SITE_ROOTS = ["https://www.bcs-express.ru", "https://bcs-express.ru"]
    HOMEPAGES = [r + "/" for r in SITE_ROOTS]
    
    def _log(logger, msg: str):
        (logger.info if logger else print)(msg)
    
    def _clean_html(text: str) -> str:
        return re.sub(r"<[^>]+>", "", text or "").strip()
    
    def _dedupe_keep_order(items: List[str]) -> List[str]:
        seen, out = set(), []
        for x in items:
            if x not in seen:
                seen.add(x); out.append(x)
        return out
    
    async def _fetch(client: httpx.AsyncClient, url: str, headers: dict, logger) -> httpx.Response:
        _log(logger, f"[BCS] GET {url}")
        resp = await client.get(url, headers=headers, follow_redirects=True)
        resp.raise_for_status()
        return resp
    
    def _parse_rss(xml: str, limit: int = 12) -> List[Tuple[str, str, str | None]]:
        items = re.findall(r"<item>(.*?)</item>", xml, re.DOTALL | re.IGNORECASE)[:limit]
        out: List[Tuple[str, str, str | None]] = []
        for chunk in items:
            m_title = re.search(r"<title>(.*?)</title>", chunk, re.DOTALL | re.IGNORECASE)
            m_link  = re.search(r"<link>(.*?)</link>", chunk,   re.DOTALL | re.IGNORECASE)
            m_pub   = re.search(r"<pubDate>(.*?)</pubDate>", chunk, re.DOTALL | re.IGNORECASE)
            title = _clean_html(m_title.group(1)) if m_title else ""
            link  = _clean_html(m_link.group(1))  if m_link  else SITE_ROOTS[0]
            pub   = _clean_html(m_pub.group(1))   if m_pub   else None
            if title:
                out.append((title, link, pub))
        return out
    
    def _parse_homepage(html: str, site_root: str, limit: int = 10) -> List[Tuple[str, str]]:
        titles_raw = re.findall(r"<h[12-6][^>]*>(.*?)</h[12-6]>", html, re.DOTALL | re.IGNORECASE)
        titles = []
        for t in titles_raw:
            clean = _clean_html(t)
            if len(clean) >= 10 and not clean.lower().startswith(("–ø–æ–¥–ø–∏—Å", "cookie", "–ø–æ–ª–∏—Ç–∏–∫")):
                titles.append(clean)
        titles = _dedupe_keep_order(titles)[:limit]

        links_raw = re.findall(r'<a[^>]+href=["\']([^"\']+)["\']', html, re.IGNORECASE)
        links = []
        for href in links_raw:
            if href.startswith("//"):
                href = "https:" + href
            elif href.startswith("/"):
                href = site_root + href
            links.append(href)

        out: List[Tuple[str, str]] = []
        for i, title in enumerate(titles):
            link = links[i] if i < len(links) else site_root
            out.append((title, link))
        return out
    
    source = "www.bcs-express.ru"

    while True:
        try:
            h_xml = get_browser_headers(accept_xml=True)
            h_html = get_browser_headers(accept_xml=False)

            items: List[Tuple[str, str, str | None]] = []
            rss_last_err = None

            # 1) RSS
            for rss_url in RSS_URLS:
                try:
                    host_root = rss_url.split("/news", 1)[0]
                    h = dict(h_xml); h["Referer"] = host_root + "/"
                    resp = await _fetch(httpx_client, rss_url, h, logger)
                    xml = resp.text
                    _log(logger, f"[BCS] RSS HTTP {resp.status_code}, bytes={len(resp.content)}")
                    candidates = _parse_rss(xml, limit=12)
                    if candidates:
                        items = candidates
                        break
                except httpx.HTTPStatusError as e:
                    rss_last_err = e
                    code = e.response.status_code
                    _log(logger, f"[BCS] RSS try fail: {code} {e}")
                    if code in (403, 406):
                        await asyncio.sleep(0.2)
                        try:
                            alt = get_browser_headers(accept_xml=True)
                            alt["Referer"] = host_root + "/"
                            alt["Sec-Fetch-Site"] = "same-origin"
                            alt["Sec-Fetch-Mode"] = "no-cors"
                            resp = await _fetch(httpx_client, rss_url, alt, logger)
                            candidates = _parse_rss(resp.text, limit=12)
                            if candidates:
                                items = candidates
                                break
                        except Exception as e2:
                            rss_last_err = e2
                    await asyncio.sleep(0.25)
                except Exception as e:
                    rss_last_err = e
                    _log(logger, f"[BCS] RSS try fail: {type(e).__name__}: {e}")
                    await asyncio.sleep(0.25)

            # 2) Fallback –Ω–∞ –≥–ª–∞–≤–Ω—É—é
            if not items:
                for site_root, home in zip(SITE_ROOTS, HOMEPAGES):
                    try:
                        h = dict(h_html); h["Referer"] = site_root + "/"
                        resp = await _fetch(httpx_client, home, h, logger)
                        html = resp.text
                        _log(logger, f"[BCS] HOME HTTP {resp.status_code}, bytes={len(resp.content)}")
                        hp = _parse_homepage(html, site_root, limit=10)
                        if hp:
                            items = [(t, l, None) for (t, l) in hp]
                            break
                    except Exception as e:
                        _log(logger, f"[BCS] HOME fail on {site_root}: {type(e).__name__}: {e}")
                        await asyncio.sleep(0.25)

                if not items and rss_last_err:
                    _log(logger, f"[BCS] RSS last error: {type(rss_last_err).__name__}: {rss_last_err}")

            # 3) –ü—É–±–ª–∏–∫—É–µ–º —Ç–æ–ª—å–∫–æ —Å–≤–µ–∂–µ–µ –∏ –±–µ–∑ –¥—É–±–ª–µ–π
            for title, link, pub_date_str in items:
                try:
                    if not date_checker.check_news_date(text=title, link=link, raw_date_str=pub_date_str, window_hours=12, logger=logger):
                        continue
                    head = title[:n_test_chars].strip()
                    if head in posted_q:
                        _log(logger, f"[BCS] skip duplicate: {head}")
                        continue
                    post = f"<b>{source}</b>\n{link}\n{title}"
                    if send_message_func:
                        await send_message_func(post)
                    else:
                        _log(logger, post + "\n")
                    posted_q.appendleft(head)
                except Exception as e:
                    _log(logger, f"[BCS] item error: {type(e).__name__}: {e}")
                    continue

        except httpx.ConnectTimeout as e:
            _log(logger, f"[BCS] ConnectTimeout: {e}")
            if error_callback:
                error_msg = f'üö® <b>BCS Parser Error</b>\n\n‚ùå <b>Source:</b> {source}\n‚ùå <b>Error:</b> ConnectTimeout\nüìù <b>Details:</b> {str(e)[:200]}'
                await error_callback(error_msg)
        except httpx.ReadTimeout as e:
            _log(logger, f"[BCS] ReadTimeout: {e}")
            if error_callback:
                error_msg = f'üö® <b>BCS Parser Error</b>\n\n‚ùå <b>Source:</b> {source}\n‚ùå <b>Error:</b> ReadTimeout\nüìù <b>Details:</b> {str(e)[:200]}'
                await error_callback(error_msg)
        except httpx.HTTPStatusError as e:
            _log(logger, f"[BCS] HTTPStatusError: {e.response.status_code} {e}")
            if error_callback:
                error_msg = f'üö® <b>BCS Parser Error</b>\n\n‚ùå <b>Source:</b> {source}\n‚ùå <b>Error:</b> HTTP {e.response.status_code}\nüìù <b>Details:</b> {str(e)[:200]}'
                await error_callback(error_msg)
        except Exception as e:
            _log(logger, f"[BCS] unexpected error: {type(e).__name__}: {e}")
            if error_callback:
                error_msg = f'üö® <b>BCS Parser Error</b>\n\n‚ùå <b>Source:</b> {source}\n‚ùå <b>Error:</b> {type(e).__name__}\nüìù <b>Details:</b> {str(e)[:200]}'
                await error_callback(error_msg)

        await asyncio.sleep(max(0.5, timeout - random.uniform(0, 0.5)))
